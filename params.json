{"name":"GitHub Large Code Analysis (ghlca)","tagline":"What can be discovered if you download thousands of GitHub repositories?","body":"## Motivation\r\nMy motivation for starting the GitHub Large Code Analysis project was not in fact the [Third Annual GitHub Data Challenge](https://github.com/blog/1864-third-annual-github-data-challenge), instead around January 2014 I was interested in doing some sort of BigData type analysis, however I had almost no idea where to start. I came up with the idea to download a bunch of repositories from GitHub and see what I could find by analyzing  all of that code. Of course, like most of my ideas, I promptly forgot about it after getting really excited about it for a week and then realized it would take real work. Later I was reading [Hacker News](news.ycombinator.com) and came across an article about [Cgrep](http://awgn.github.io/cgrep/), which sparked my interest in the ghlca project again. I also came across another article about [The impact of language choice on github projects](http://corte.si/posts/code/devsurvey/), which was very much like part of what I wanted to do in the ghlca project. The final thing which got me working on the ghlca project was when a coworker asked if I had heard about the [Third Annual GitHub Data Challenge](https://github.com/blog/1864-third-annual-github-data-challenge), which finally gave me a deadline.\r\n\r\n## Getting Started\r\nThe first problem that needed solving was to determine which github repositories were the most 'valuable' repositories. I determined three ways of judging the 'value' of a repository using the dataset on Google BigQuery. Either by the most number of watchers, the most number of forks, or the highest value for watchers * forks. For the competition, I simply used about the top 5000 repositories with the most watchers, since I was under a time limit. The following was the code used to get those repositories, which I had found this on StackOverflow (unfortunately, I can't find it anymore).\r\n```\r\nSELECT\r\n  *\r\nFROM [publicdata:samples.github_timeline] a\r\nJOIN EACH\r\n  (\r\n     SELECT MAX(created_at) as max_created, repository_url \r\n     FROM [publicdata:samples.github_timeline]\r\n     GROUP EACH BY repository_url\r\n  ) b\r\n  ON \r\n  b.max_created = a.created_at and\r\n  b.repository_url = a.repository_url\r\nORDER BY repository_watchers desc\r\nLIMIT 5000\r\n```\r\n## Observations of Note\r\nWhile doing the initial development of my word counting python scripts, I used the [Apache Cassandra](https://github.com/apache/cassandra) repository for testing (filtering for only the Java files), and discovered that the number of left and right parenthesis did not match, instead there were more left parenthesis than right. This could be explained by frown faces in the comments, however there were also more left curly braces '{' than right curly braces '}', and more left square brackets '[' than right square brackets ']'.\r\n\r\n## Most Common Languages\r\nTODO: List of the most common languages by file count, character count, sloc, and possibly loc.\r\n\r\n## Character Frequency\r\nTODO: Graph of most common characters\r\nTODO: Graph of characters most specific to each of the most common languages\r\n\r\n## Mismatched Brackets\r\nWhile testing the character counting code against the [Apache Cassandra](https://github.com/apache/cassandra) repository I discovered that all of the types of brackets did not have a matching number of left and right occurrences.\r\nTODO: Get ratios of each of the bracket types for the left and right versions\r\nTODO: Get percentage of files that have mismatched brackets for each type of bracket\r\nTODO: Get percentage of repositories that have a file with mismatched brackets for each type of bracket\r\n\r\n### Angular Brackets\r\nAngular brackets `<>` are the easiest to explain why they did not match, since they are often used without matching for greater than or less than comparisons e.g. `for(int i=0; i < 10; i++){ System.out.println(i); }` or for various other things, like type annotations in Haskell e.g. `fib :: Int -> Integer` (source: [Haskell Wikipedia Page](http://en.wikipedia.org/wiki/Haskell_%28programming_language%29)).\r\nTODO: Insert link to a file that has a mismatched number of left and right angular brackets from apache cassandra\r\n\r\n### Parentheses\r\nParentheses are also quite easy to explain, since they are commonly used without matching in emoticons such as `:)` or `:(`.\r\nTODO: Determine any other causes of parentheses mismatch by filtering out files with mismatched parentheses when ignoring all of the types of emoticons in the [Emoticon Wikipedia Page](http://en.wikipedia.org/wiki/List_of_emoticons).\r\nTODO: Insert a link to a file with mismatched parentheses from apache cassandra\r\n\r\n### Square Brackets\r\nI find it surprising that left and right square brackets don't match, even for a single repository like the Apache Cassandra repository. They are not commonly used in emoticons in my experience, and I cannot think of any language constructs that would have a mismatched square bracket (and while testing against apache/cassandra, I only used the java source code files).\r\n\r\nWhile naming this section, I discovered that parentheses `()`, curly braces `{}`, square brackets `[]` (what I used to refer to as brackets), and angular brackets `<>` are all types of brackets (source: [Math is Fun Brackets](http://www.mathsisfun.com/algebra/brackets.html)).\r\n\r\n## Bugs Encountered\r\nI encountered quite a few bugs in the software I was using, from bugs in python, to bugs in the github-linguist library for determining the language of source code files. I will be submitting bug reports for github linguist and will likely do some work on fixing them, since it would be a great way for me to contribute to an open source application in a meaningful way. In python I encountered some segfaults (so far I have had python segfault on my twice) but am unable to reproduce the segfault consistently. I also encountered issues while reading source code files with unicode, since many source files have invalid unicode characters. I would ilke a way to read a file as text, but to be able to either skip the characters that are invalid unicode, or to treat them in some special way. I believe most of these files were for tests, and some of them were for testing what happens on invalid input.\r\n\r\n### Acknowledgements\r\nI would like to thank Aldo Cortesi (@cortesi) for his post on his work downloading a bunch of github repositories at [The impact of language choice on github projects](http://corte.si/posts/code/devsurvey/).\r\nI used the [Convert JSON to CSV](http://www.convertcsv.com/json-to-csv.htm) site for converting my results from scripts that were in JSON to CSV so that I could import it into LibreOffice Calc for making graphs.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}