{"name":"GitHub Large Code Analysis","tagline":"What can be discovered with thousands of GitHub repositories?","body":"## Motivation\r\nMy motivation for starting the GitHub Large Code Analysis project was not in fact the [Third Annual GitHub Data Challenge](https://github.com/blog/1864-third-annual-github-data-challenge), instead around January 2014 I was interested in doing some sort of BigData type analysis, however I had almost no idea where to start. I came up with the idea to download a bunch of repositories from GitHub and see what I could find by analyzing  all of that code. Of course, like most of my ideas, I promptly forgot about it after getting really excited about it for a week and then realized it would take real work. Later I was reading [Hacker News](news.ycombinator.com) and came across an article about [Cgrep](http://awgn.github.io/cgrep/), which sparked my interest in the ghlca project again. I also came across another article about [The impact of language choice on github projects](http://corte.si/posts/code/devsurvey/), which was very much like part of what I wanted to do in the ghlca project. The final thing which got me working on the ghlca project was when a coworker asked if I had heard about the [Third Annual GitHub Data Challenge](https://github.com/blog/1864-third-annual-github-data-challenge), which finally gave me a deadline.\r\n\r\n## Getting Started\r\nThe first problem that needed solving was to determine which github repositories were the most 'valuable' repositories. I determined three ways of judging the 'value' of a repository using the dataset on Google BigQuery. Either by the most number of watchers, the most number of forks, or the highest value for watchers * forks. For the competition, I simply used about the top 5000 repositories with the most watchers, since I was under a time limit. The following was the code used to get those repositories, which I had found this on StackOverflow (unfortunately, I can't find it anymore).\r\n```\r\nSELECT\r\n  *\r\nFROM [publicdata:samples.github_timeline] a\r\nJOIN EACH\r\n  (\r\n     SELECT MAX(created_at) as max_created, repository_url \r\n     FROM [publicdata:samples.github_timeline]\r\n     GROUP EACH BY repository_url\r\n  ) b\r\n  ON \r\n  b.max_created = a.created_at and\r\n  b.repository_url = a.repository_url\r\nORDER BY repository_watchers desc\r\nLIMIT 5000\r\n```\r\n## Observations of Note\r\nWhile doing the initial development of my word counting python scripts, I used the [Apache Cassandra](https://github.com/apache/cassandra) repository for testing (filtering for only the Java files), and discovered that the number of left and right parenthesis did not match, instead there were more left parenthesis than right. This could be explained by frown faces in the comments, however there were also more left curly braces '{' than right curly braces '}', and more left square brackets '[' than right square brackets ']'.\r\n\r\n## Most Common Languages\r\nTODO: List of the most common languages by file count, character count, sloc, and possibly loc.\r\n\r\n## Word and Character Frequency\r\nFor the GitHub competition, I decided to focus on word and character counts of the files. I found some very interesting results from this data.\r\nOne of the issues I ran into was determining how to count 'words'. Following the literature only for how to count words in python, the python [Counter](https://docs.python.org/3.4/library/collections.html#collections.Counter) was used after the text had been split into an array of words using `text.split()`. I determined that this caused some unexpected complications when dealing with code due to how source code is parsed, for example that curly braces and such can sometimes be separated with whitespace from a variable, and sometimes not. With some more research, I determined that I should also use the regex word character class by running `re.split(\"[\\W]+\", text)` to split the text into words. I also counted the characters since I determined that it may be useful after I saw a mismatch in the number of left and right curly brackets when counting words with `text.split()`, and realized I would have to do a character count to determine if there was a mismatch.\r\n\r\n### Character Frequency\r\nTODO: Graph of most common characters\r\nTODO: Graphs of characters most specific to each of the most common languages\r\n\r\n### Word Frequency\r\nTODO: Graph of most common words\r\nTODO: Graphs of words most specific to each of the most common languages\r\n\r\n### Whitespace Separated Word Frequency\r\nI also did word counts using the words produced by calling text.split() against the text of a file, however it produced the least valuable data of each of character frequency and words defined by the word character class in regex. However here are some graphs of that data.\r\nTODO: Graph of most common whitespace separated words\r\nTODO: Graphs of whitespace separated words specific to each of the most common languages\r\n\r\n## Mismatched Brackets\r\nWhile testing the character counting code against the [Apache Cassandra](https://github.com/apache/cassandra) repository I discovered that all of the types of brackets did not have a matching number of left and right occurrences.\r\nTODO: Get ratios of each of the bracket types for the left and right versions\r\nTODO: Get percentage of files that have mismatched brackets for each type of bracket\r\nTODO: Get percentage of repositories that have a file with mismatched brackets for each type of bracket\r\n\r\n### Angular Brackets\r\nAngular brackets `<>` are the easiest to explain why they did not match, since they are often used without matching for greater than or less than comparisons e.g. `for(int i=0; i < 10; i++){ System.out.println(i); }` or for various other things, like type annotations in Haskell e.g. `fib :: Int -> Integer` (source: [Haskell Wikipedia Page](http://en.wikipedia.org/wiki/Haskell_%28programming_language%29)).\r\nTODO: Insert link to a file that has a mismatched number of left and right angular brackets from apache cassandra\r\n\r\n### Parentheses\r\nParentheses are also quite easy to explain, since they are commonly used without matching in emoticons such as `:)` or `:(`.\r\nTODO: Determine any other causes of parentheses mismatch by filtering out files with mismatched parentheses when ignoring all of the types of emoticons in the [Emoticon Wikipedia Page](http://en.wikipedia.org/wiki/List_of_emoticons).\r\nTODO: Insert a link to a file with mismatched parentheses from apache cassandra\r\n\r\n### Square Brackets\r\nI find it surprising that left and right square brackets don't match, even for a single repository like the Apache Cassandra repository. They are not commonly used in emoticons in my experience, and I cannot think of any language constructs that would have a mismatched square bracket (and while testing against apache/cassandra, I only used the java source code files).\r\n\r\nWhile naming this section, I discovered that parentheses `()`, curly braces `{}`, square brackets `[]` (what I used to refer to as brackets), and angular brackets `<>` are all types of brackets (source: [Math is Fun Brackets](http://www.mathsisfun.com/algebra/brackets.html)).\r\n\r\n## Repositories Used\r\nTODO: Link to a list of the repositories used and the commit hash that they were at when you got them\r\n\r\n## Bugs Encountered\r\nI encountered quite a few bugs in the software I was using, from bugs in python, to bugs in the github-linguist library for determining the language of source code files. I will be submitting bug reports for github linguist and will likely do some work on fixing them, since it would be a great way for me to contribute to an open source application in a meaningful way. In python I encountered some segfaults (so far I have had python segfault 3 times) but am unable to reproduce the segfault consistently. I also encountered issues while reading source code files with unicode, since many source files have invalid unicode characters. I would ilke a way to read a file as text, but to be able to either skip the characters that are invalid unicode, or to treat them in some special way. I believe most of these files were for tests, and some of them were for testing what happens on invalid input.\r\n\r\n## Acknowledgements\r\nI would like to thank Aldo Cortesi (@cortesi) for his post on his work downloading a bunch of github repositories at [The impact of language choice on github projects](http://corte.si/posts/code/devsurvey/).\r\nI used the [Convert JSON to CSV](http://www.convertcsv.com/json-to-csv.htm) site for converting my results from scripts that were in JSON to CSV so that I could import it into LibreOffice Calc for making graphs.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}